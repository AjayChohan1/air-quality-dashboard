{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **ETL PROCESS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Description\n",
        "\n",
        "This notebook performs an ETL (Extract, Transform, Load) process on US air pollution data from 2000 to 2016. It imports the raw CSV, cleans the data by removing missing values and unnecessary columns, renames columns for clarity, and prepares a sample for analysis. The final processed data is saved for further use in analytics or machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fetch data and save it as raw data file and upload it to the workspace. Take the data through the ETL process to clean it.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Raw CSV data file.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* This notebook will hope to generate a clean CSV file of the data. \n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* This dataset was sourced from Kaggle and contains data regarding air pollution quality in the US in from 2000 to 2016.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory\n",
        "* When you restart the kernel (and clear outputs, if necessary) always be certain that these 3 cells run in order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract and read the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import feature_engine as fe \n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "print(\"All packages imported successfully!\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"Scikit-learn version: {sk.__version__}\")\n",
        "print(f\"Feature-engine version: {fe.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read data and return full DataFrame with shape to make sure everything is working\n",
        "# Also return the data shape\n",
        "df = pd.read_csv(\"inputs/pollution_us_2000_2016.zip\", compression=\"zip\")\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Return the first five values of the DataFrame for future observation purposes where necessary\n",
        "# Also return the data types\n",
        "df.head()\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop missing values from df1\n",
        "df = df.dropna()\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "print(\"Missing values dropped from df.\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Begin to transform the data, creating transformers and the pipeline code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the current columns\n",
        "print(\"Data loaded successfully!\")\n",
        "print(\"Available columns:\")\n",
        "print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check minimum values for numerical columns only\n",
        "print(\"Minimum values for numerical columns:\")\n",
        "print(df.select_dtypes(include=\"number\").min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check maximum values for numerical columns only\n",
        "print(\"Maximum values for numerical columns:\")\n",
        "print(df.select_dtypes(include=\"number\").max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicated values and return their sum\n",
        "print(\"Data loaded successfully!\")\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for null values in each column and return their sum\n",
        "print(\"Data loaded successfully!\") \n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print column names and their data types for df1\n",
        "print(\"Column names:\")\n",
        "print(df1.columns.tolist())\n",
        "print(f\"DataFrame shape: {df1.shape}\")\n",
        "print(df1.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop \"Unnamed: 0\" column from df1\n",
        "# Code drops all columns with string \"Unnamed\"\n",
        "df1 = df1.loc[:, ~df1.columns.str.contains(\"^Unnamed\")]\n",
        "print(\"'Unnamed: 0' column dropped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print column names and their data types for df1\n",
        "print(\"Column names:\")\n",
        "print(df1.columns.tolist())\n",
        "print(f\"DataFrame shape: {df1.shape}\")\n",
        "print(df1.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rename \"Site Num\" column to \"Site Number\" in df1\n",
        "df1 = df1.rename(columns={\"Site Num\": \"Site Number\"})\n",
        "print(\"Column 'Site Num' renamed to 'Site Number'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print column names and their data types for df1\n",
        "print(\"Column names:\")\n",
        "print(df1.columns.tolist())\n",
        "print(f\"DataFrame shape: {df1.shape}\")\n",
        "print(df1.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add \"Date\", \"Year\" and \"Month\" columns using the \"Date Local\" column\n",
        "# Make sure the new \"Date\" column is in \"datetime\" format\n",
        "df1[\"Date\"] = pd.to_datetime(df1[\"Date Local\"])\n",
        "df1[\"Year\"] = df1[\"Date\"].dt.year\n",
        "df1[\"Month\"] = df1[\"Date\"].dt.month\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop \"Date Local\" column from df1\n",
        "df1 = df1.drop(columns=[\"Date Local\"])\n",
        "print(\"'Date Local' column dropped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print column names and their data types for df1\n",
        "print(\"Column names:\")\n",
        "print(df1.columns.tolist())\n",
        "print(f\"DataFrame shape: {df1.shape}\")\n",
        "print(df1.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop all rows where the 'City' column is 'Not in a city'\n",
        "df1 = df1[df1['City'] != 'Not in a city']\n",
        "print(\"Rows with 'Not in a city' in the City column dropped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract a random, fractioned sample of the data of 2500 values for analytic purposes\n",
        "# Also return the new data shape\n",
        "df1 = df.sample(frac=0.005722, random_state=10)\n",
        "print(f\"DataFrame shape: {df1.shape}\")\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the data to the necessary file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the processed datasets\n",
        "# df1.to_csv(\"outputs/analysis.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are a few insights from this notebook:\n",
        "\n",
        "* The air pollution dataset covers US data from 2000 to 2016 and includes multiple pollutants and site information.\n",
        "* Data cleaning steps removed missing values and unnecessary columns, improving data quality for analysis.\n",
        "* The column \"Site Num\" was renamed to \"Site Number\" for clarity.\n",
        "* A random sample of 2,500 rows was extracted for efficient analysis.\n",
        "* The cleaned data is ready for further statistical analysis, visualization, or machine learning tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The dataset was very large and introduced commit conflicts to origin. This was resolved be sending it to a zip file and reintroducing it to the workspace.\n",
        "* Also, working against time constraints found itself difficult, though the necessary data was manifested"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This ETL session was quite intriguing. Time was spent trying to deduce the right dataset to work with, but in the end the dataset was generated and loaded to further analyze."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
